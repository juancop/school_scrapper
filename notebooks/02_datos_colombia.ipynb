{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrJHUQ2XcL7s"
      },
      "source": [
        "# Datos Colombia\n",
        "\n",
        "En este Notebook se implementa un scraper para Datos Colombia. Esta página se vuelve más complicada debido a que tiene validaciones contra búsquedas automatizadas.\n",
        "\n",
        "Este código permitirá realizar el almacenamiento de la información y la guardará en Google Drive para evitar temas de desconexión por runtime.\n",
        "\n",
        "La página cuenta con 3137 vistas con 13 colegios, lo que permitiría extraer información de 40781 instituciones educativas en todo el país."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-Gd93sgRSGM"
      },
      "outputs": [],
      "source": [
        "from urllib.parse import unquote\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import requests\n",
        "import copy\n",
        "import time\n",
        "import os\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEDo4XV1RZHk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o45H7y_2cZkM"
      },
      "outputs": [],
      "source": [
        "def extract_body_for_urls(idx):\n",
        "  \"\"\"\n",
        "  Permite extraer las URLs de los colegios que se encuentran listados en la página.\n",
        "\n",
        "  Params\n",
        "  --------\n",
        "    idx (int):\n",
        "      Indicador del número de página. De 1 a 3137.\n",
        "\n",
        "\n",
        "  Returns\n",
        "  --------\n",
        "\n",
        "    schools (dict):\n",
        "      Un diccionario con el nombre de cada colegio de la página y su url\n",
        "      {name:url}\n",
        "\n",
        "  \"\"\"\n",
        "  index_url = f'https://datoscolombia.com/escuelas-colegios/page/{idx}'\n",
        "  base_url = 'https://datoscolombia.com'\n",
        "\n",
        "  headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
        "\n",
        "  # Se debe hacer el request como si fuera un agente, no automático.\n",
        "  response = requests.get(index_url, headers=headers)\n",
        "  soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "  # La página contiene links de cada colegio\n",
        "  school_links = soup.find_all('a', href=lambda href: href and href.startswith('/escuelas-colegios/'))\n",
        "  schools = {}\n",
        "  for link in school_links:\n",
        "      href = base_url + link.get('href')\n",
        "      name = link.get_text()\n",
        "      schools[name] = href\n",
        "\n",
        "  return schools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRTE1CPqRZ-k"
      },
      "outputs": [],
      "source": [
        "def extract_school_information(school_url):\n",
        "  \"\"\"\n",
        "  Extrae la información de cada colegio según lo disponible en la página.\n",
        "\n",
        "  Params\n",
        "  --------\n",
        "    school_url (str):\n",
        "      La url del colegio del que se extraerá la información\n",
        "\n",
        "  Returns\n",
        "  --------\n",
        "    school_info (dict):\n",
        "      Un diccionario que tiene toda la información extraída\n",
        "\n",
        "  \"\"\"\n",
        "  headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
        "  response = requests.get(school_url, headers=headers)\n",
        "  soup = BeautifulSoup(response.text, 'html.parser')\n",
        "  info_divs = soup.find_all('div', class_='info-est')\n",
        "\n",
        "  # Diccionario de Información Extraída\n",
        "  school_info = {}\n",
        "\n",
        "  for div in info_divs:\n",
        "      info_fields = div.find_all('p')\n",
        "\n",
        "      for field in info_fields:\n",
        "          label = field.find('label', class_='title-field')\n",
        "          if label:\n",
        "              label_text = label.get_text(strip=True)\n",
        "              field_copy = copy.copy(field)\n",
        "              for l in field_copy.find_all('label', class_='title-field'):\n",
        "                  l.decompose()\n",
        "\n",
        "              value_text = field_copy.get_text(strip=True)\n",
        "              school_info[label_text] = value_text\n",
        "\n",
        "  return school_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yByHC-ZbglHp"
      },
      "outputs": [],
      "source": [
        "output_file = 'drive/MyDrive/datos_colombia_information.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_VRzfesg9iA"
      },
      "outputs": [],
      "source": [
        "\n",
        "if os.path.isfile(output_file):\n",
        "  df = pd.read_csv(output_file)\n",
        "  max_pages = df.page_num.max()\n",
        "  column_order = df.columns.values.tolist()\n",
        "else:\n",
        "  max_pages = 1\n",
        "  column_order = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4CrWhOKRtvK",
        "outputId": "3a33a719-1f1e-4252-b7ee-0e95d196f0c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 34/1439 [59:17<40:30:30, 103.79s/it]"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "for idx in tqdm(range(max_pages, 3138)):\n",
        "  school_urls = extract_body_for_urls(idx)\n",
        "  time.sleep(random.uniform(1, 10))\n",
        "\n",
        "  # Lista para almacenar la información\n",
        "  complete_school_extraction = []\n",
        "\n",
        "  for school_name, school_url in school_urls.items():\n",
        "    time.sleep(1)\n",
        "    school_info = extract_school_information(school_url)\n",
        "    school_info['url'] = school_url\n",
        "    school_info['name'] = school_name\n",
        "    school_info['page_num'] = idx\n",
        "    time.sleep(random.uniform(1, 5))\n",
        "    complete_school_extraction.append(school_info)\n",
        "\n",
        "    # Estructuración en Data Frame\n",
        "    df = pd.DataFrame(complete_school_extraction)\n",
        "\n",
        "    new_columns = [c for c in df.columns if c not in column_order]\n",
        "    column_order.extend(new_columns)\n",
        "\n",
        "    df = df.reindex(columns=column_order)\n",
        "\n",
        "\n",
        "\n",
        "  # Append si existe, y creación en primera iteración\n",
        "  if os.path.isfile(output_file):\n",
        "    df.to_csv(output_file, mode='a', header=False, index=False)\n",
        "  else:\n",
        "    df.to_csv(output_file, mode='w', header=True, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVh5xbKmjUWU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}